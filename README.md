# About 
This project study the effect of attention head(s) ablation on the performance of mBERT, fine-tuned on XNLI dataset, in both zero-shot crosslingual transfer learning setting (fine-tune on Enlish data only, evaluate on all languages), and normal setting (fine-tune on all languages and evaluate on all languages).


